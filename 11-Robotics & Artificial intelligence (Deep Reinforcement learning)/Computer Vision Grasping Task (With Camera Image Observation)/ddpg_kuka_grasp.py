# -*- coding: utf-8 -*-
"""DDPG_KUKA_Grasp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAD8FRhW10eEKeLHaPl3XUJvuAhGRjx7
"""

!pip install pybullet

import torch
import torch.nn as nn
import numpy as np 
import matplotlib.pyplot as plt
from PIL import Image
import imageio_ffmpeg
from pybullet_envs.bullet.kuka_diverse_object_gym_env import KukaDiverseObjectEnv
from gym import spaces
import pybullet as p
import torchvision.transforms as T
from math import sqrt
from google.colab import drive
import gc

class Buffer():
  def __init__(self,Buffer_size):
    self.Buffer_size = Buffer_size
    self.states = []
    self.actions = []
    self.rewards = []
    self.next_states = []
    self.dones = []
  
  def Add_(self,state,action,reward,next_state,done):
    if len(self.states) > self.Buffer_size-1: 
      del(self.states[0])
      del(self.actions[0])
      del(self.rewards[0])
      del(self.next_states[0])
      del(self.dones[0])
    self.states.append(state)
    self.actions.append(action)
    self.rewards.append(reward)
    self.next_states.append(next_state)
    self.dones.append(done)
    return self.states, self.actions, self.rewards, self.next_states, self.dones

class ConvNet(nn.Module):
  def __init__(self,state_size,action_dim):
    super(ConvNet,self).__init__()
    self.state_size = (state_size[0],state_size[1])
    #Convolutional neural network
    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
    self.bn1 = nn.BatchNorm2d(16)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
    self.bn2 = nn.BatchNorm2d(32)
    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
    self.bn3 = nn.BatchNorm2d(32)
   
    self.Relu = nn.ReLU()

    def conv2d_size_out(size, kernel_size = 5, stride = 2):
      return (size - (kernel_size - 1) - 1) // stride  + 1

    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[0])))
    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[1])))
    linear_input_size = convh * convw * 32

    self.sharedlayers = nn.Sequential(
        nn.Linear(linear_input_size,128),
        nn.ReLU())
    
  def sharedforward(self,state):
    Out = self.Relu(self.bn1(self.conv1(state)))
    Out = self.Relu(self.bn2(self.conv2(Out)))
    Out = self.Relu(self.bn3(self.conv3(Out)))
    Out = self.sharedlayers(Out.view(Out.size(0), -1))
    return Out

class Actor(ConvNet): 
  def __init__(self,state_size,action_dim):
    super().__init__(state_size,action_dim)
    #Actor network 
    self.actor = nn.Sequential(
        nn.Linear(128,128),
        nn.ReLU(),
        nn.Linear(128,128),
        nn.Tanh())
  def forward(self,state):
    action = self.actor(self.sharedforward(state))
    return action 

class Critic(ConvNet):
  def __init__(self,state_size,action_dim): 
    super().__init__(state_size,action_dim)
    #Critic network 
    self.critic_action = nn.Sequential(
        nn.Linear(action_dim,128),
        nn.ReLU())
    
    self.critic_state = nn.Sequential(
        nn.Linear(128,128),
        nn.ReLU())
    
    self.critic = nn.Sequential(
        nn.Linear(256,128),
        nn.ReLU(),
        nn.Linear(128,1),
        nn.ReLU())
    
  def forward(self,state,action):
    state_critic = self.critic_state(self.sharedforward(state))
    action_critic = self.critic_action(action)
    value = self.critic(torch.cat((state_critic, action_critic), dim=1)) 
    return value

class DDPG():
  def __init__(self,state_size,action_dim,Actor_learning_rate,Critic_learning_rate,discount_factor,buffer_size):
    self.discount_factor = discount_factor
    self.Actor_learning_rate = Actor_learning_rate
    self.Critic_learning_rate = Critic_learning_rate
    self.state_size = state_size 
    self.action_dim = action_dim
    self.ActorTarget = Actor(self.state_size,self.action_dim)
    self.Actor = Actor(self.state_size,self.action_dim)
    self.CriticTarget = Critic(self.state_size,self.action_dim)
    self.Critic = Critic(self.state_size,self.action_dim)
    self.Buffer = Buffer(buffer_size)
    self.MSE = nn.MSELoss()
    self.actor_optimizer = torch.optim.Adam(self.Actor.parameters(),lr = self.Actor_learning_rate)
    self.critic_optimizer = torch.optim.Adam(self.Critic.parameters(),lr = self.Critic_learning_rate)
  
  def ClearBuffer(self):
     self.Buffer.states = []
     self.Buffer.actions = []
     self.Buffer.rewards = []
     self.Buffer.next_states = []
     self.Buffer.dones = []
  
  #Ornstein-Uhlenbeck Noise
  def Noise(self,noise,noise_std,noise_decay_rate):
    MeanAttractionConstant = 0.15
    Mean = 0 
    Ts = 0.1
    noise = noise + MeanAttractionConstant*(Mean - noise)*Ts + noise_std*round(np.random.randn(),4)*sqrt(Ts)
    noise_std = noise_std*(1 - noise_decay_rate)
    return noise,noise_std

  def step(self,env,state,noise):
    action = self.Actor.forward(state)
    #Action used to learn 
    action = action + noise
    _,reward,done,_ = env.step(action[0])
    if reward: 
     reward = reward*100
    else: 
      reward = -1
    next_state = get_screen(env)
    self.Buffer.Add_(state,action,reward,next_state,done)
    return self.Buffer.states,self.Buffer.actions,self.Buffer.rewards,self.Buffer.next_states,self.Buffer.dones
  
  #Selecting shuffle of mini batch in the buffer memory
  def Mini_batch(self,states,actions,rewards,next_states,dones,Mini_Batch_Size):
    batch_states = []
    batch_actions = []
    batch_rewards = []
    batch_next_states = []
    batch_dones = []
    indxs = [i for i in range(len(rewards))]
    np.random.shuffle(indxs)
    if len(rewards) > Mini_Batch_Size :
      batch_states = [states[ind] for ind in indxs[0:64]] 
      batch_actions = [actions[ind] for ind in indxs[0:64]] 
      batch_rewards = [rewards[ind] for ind in indxs[0:64]] 
      batch_next_states = [next_states[ind] for ind in indxs[0:64]]
      batch_dones = [dones[ind] for ind in indxs[0:64]]
    else: 
      batch_states = states
      batch_actions = actions
      batch_rewards = rewards
      batch_next_states = next_states
      batch_dones = dones
    batch_states = torch.cat(batch_states, dim=0)
    batch_actions = torch.cat(batch_actions, dim=0)
    batch_rewards = torch.tensor(np.array(batch_rewards).reshape((-1, 1)))
    batch_next_states = torch.cat(batch_next_states, dim=0)
    return batch_states,batch_actions,batch_rewards,batch_next_states,batch_dones

  def update(self,batch_states,batch_actions,batch_rewards,batch_next_states,batch_dones):
    actions_target = self.ActorTarget.forward(batch_next_states).detach()
    values_target = self.CriticTarget.forward(batch_next_states,actions_target).detach()
    #Target value 
    for i,done in enumerate(batch_dones):
      if done: 
        values_target[i] = 0 
         
    y = batch_rewards.unsqueeze(0) + self.discount_factor * values_target
    
    #Critic update
    detached_action = batch_actions.detach()
    Values_critic = self.Critic.forward(batch_states,detached_action)
    Critic_LOSS = self.MSE(y,Values_critic)  
    #update the critic newtwork
    self.critic_optimizer.zero_grad()
    Critic_LOSS.backward(retain_graph = True)
    torch.nn.utils.clip_grad_norm_(self.Critic.parameters(),10)
    self.critic_optimizer.step()
  
  
    #Actor update
    actions = self.Actor.forward(batch_states)
    Values_critic = self.Critic.forward(batch_states,actions)
    Actor_LOSS = -torch.mean((Values_critic).pow(2))
    #update the actor network
    self.actor_optimizer.zero_grad()
    Actor_LOSS.backward(retain_graph = True)
    torch.nn.utils.clip_grad_norm_(self.Actor.parameters(),10)
    self.actor_optimizer.step()
    return Actor_LOSS, Critic_LOSS

  def update_target(self):
    self.ActorTarget.load_state_dict(self.Actor.state_dict())
    self.CriticTarget.load_state_dict(self.Critic.state_dict())

#Preprocessing of images
resize = T.Compose([T.ToPILImage(),
                    T.Resize(40, interpolation=Image.CUBIC),
                    T.ToTensor()])

def get_screen(env):
    screen = env._get_observation().transpose((2, 0, 1))
    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
    screen = torch.from_numpy(screen)
    return resize(screen).unsqueeze(0).to(device)

env = KukaDiverseObjectEnv(renders=False, isDiscrete=False, removeHeightHack=False, maxSteps = 25 )
env.cid = p.connect(p.DIRECT)
action_space = spaces.Box(low=-1, high=1, shape=(5,1))
# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
env.reset()
drive.mount('/content/drive')

state = get_screen(env)
state_size = (state.shape[2],state.shape[3])
action_dim = env.action_space.shape[0]

max_episodes = 1000000
max_steps = 10
saving_rate = 5000
Target_updating_rate = 3
Ploting_rate = 100
MiniBatch_size = 264

noise = 0
noise_std = sqrt(0.01)

d_rewards=[]
d_ActorLOSS=[]
d_CriticLOSS=[]
AL = 0
CL = 0
s = 0
d = 0
k = 0

Policy = DDPG(state_size,action_dim,Actor_learning_rate = 1e-5,Critic_learning_rate = 1e-5,discount_factor = 0.99,buffer_size = 1e6)

if torch.cuda.is_available():
    Policy.ActorTarget.cuda()
    Policy.Actor.cuda()
    Policy.CriticTarget.cuda()
    Policy.Critic.cuda()

Policy.ClearBuffer()

for e in range(max_episodes): 
  env.reset()
  for step in range(max_steps): 
    state = get_screen(env)
    noise, noise_std = Policy.Noise(noise,noise_std,1e-4)
    states,actions,rewards,next_states,dones = Policy.step(env,state,noise)
    batch_states, batch_actions,batch_rewards, batch_next_states, batch_dones = Policy.Mini_batch(states,actions,rewards,next_states,dones,MiniBatch_size)
    Actor_Loss, Critic_Loss = Policy.update(batch_states, batch_actions.to(device),batch_rewards.to(device), batch_next_states.to(device),batch_dones)  
    Actor_Loss = Actor_Loss.detach()
    Critic_Loss = Critic_Loss.detach()
    #Update Target networks
    if (s+1)%Target_updating_rate == 0:
      Policy.update_target()
    
    #Print data
    s = s + 1 
    AL = AL + Actor_Loss
    CL = CL + Critic_Loss
    #print("episode: ",s, " reward: ",rewards[-1], " Actor Loss: ",Actor_Loss,  " Critic Loss: ",Critic_Loss) 
    if rewards[-1] == 100:
      d = d + 1

    if (e + 1) % Ploting_rate == 0 and dones[-1]:
      f = s - k
      print("episode: ",e+1, " reward: ",d, " Actor Loss: ",AL.detach().tolist()/f, " Critic Loss: ",CL.detach().tolist()/f) 
      d = 0
      AL = 0
      CL = 0
      k = s 

    #Saving data on GOOGLE DRIVE 
    if (s+1)%saving_rate==0:
      torch.save(Policy.Actor.state_dict(), '/content/drive/MyDrive/Models/Actor.pt')
      torch.save(Policy.Critic.state_dict(), '/content/drive/MyDrive/Models/Critic.pt')
      torch.save(Policy.ActorTarget.state_dict(), '/content/drive/MyDrive/Models/ActorTarget.pt')
      torch.save(Policy.CriticTarget.state_dict(), '/content/drive/MyDrive/Models/CriticTarget.pt')
      print("===============================================SAVED======================================================= \n")
    if dones[-1]: 
      break

#out1 = Policy.Critic.sharedforward(batch_states)
#out2 = Policy.Critic.critic_state(Policy.Critic.sharedforward(batch_states))
#out3 = Policy.Critic.critic_action(batch_actions)
#out4 = Policy.Critic.critic(torch.cat((out2, out3), dim=1))

#Saving data on GOOGLE DRIVE 
drive.mount('/content/drive')
if (s+1)%saving_rate==0:
  torch.save(Policy.Actor.state_dict(), '/content/drive/MyDrive/Models/Actor.pt')
  torch.save(Policy.Critic.state_dict(), '/content/drive/MyDrive/Models/Critic.pt')
  torch.save(Policy.ActorTarget.state_dict(), '/content/drive/MyDrive/Models/ActorTarget.pt')
  torch.save(Policy.CriticTarget.state_dict(), '/content/drive/MyDrive/Models/CriticTarget.pt')
  print("===============================================SAVED======================================================= \n")

state = get_screen(env)
state_size = (state.shape[2],state.shape[3])
action_dim = env.action_space.shape[0]
Policy = DDPG(state_size,action_dim,Actor_learning_rate = 1e-4,Critic_learning_rate = 1e-3,discount_factor = 0.99,buffer_size = 1e6)

Policy.Critic.state_dict()

Policy.Critic.state_dict()

torch.load(Policy.Critic.state_dict(), '/content/Critic.pt')

import torch
import io

# Define the path to the .pt file
path = 'path/to/model.pt'

# Open the file as a buffer
with open("/content/Critic.pt", 'rb') as f:
    buffer = io.BytesIO(f.read())

# Load the model from the buffer
model = torch.load(buffer)

for key, value in model.items():
    print(key)
    print(value)


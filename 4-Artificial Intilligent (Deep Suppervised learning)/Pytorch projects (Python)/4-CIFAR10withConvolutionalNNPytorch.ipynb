{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_CIFAR10withConvolutionalNNPytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kYxlj1hWNt7T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "metadata": {
        "id": "aygRevN2STYU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
      ],
      "metadata": {
        "id": "Gc-DACvdS5N1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = torchvision.datasets.CIFAR10(root='./data',train = True , transform=transform , download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data' , train=True , transform = transform, download=True)\n",
        "\n",
        "Train_loader = torch.utils.data.DataLoader(dataset=train_data , shuffle = True , batch_size = 10)\n",
        "Test_loader = torch.utils.data.DataLoader(dataset=test_data , batch_size = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIibih0DTgnR",
        "outputId": "3630eb82-cecc-46ba-8c10-4e2ec967f2ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "s0j__cPQWpTN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example \n",
        "example = iter(Train_loader)\n",
        "image,label = example.next() \n",
        "plt.imshow(image[0][0])   #normalized images\n",
        "print(classes[label[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "VRZyRtXtUQ6g",
        "outputId": "82c01cda-744e-4603-c045-450747b43632"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bird\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3,32,3)    #input_channels , output_channels , kernel_size(filter_size) , padding = 0 and stride = 1 as default   (30,30,32)\n",
        "    self.Pool1 = nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(32,64,3)  #output_chanels , outputs_channels2                 \n",
        "    self.linear1 = nn.Linear(64*13*13,64)    #64 neurons and flatten              \n",
        "    self.linear2 = nn.Linear(64,32)\n",
        "    self.linear3 = nn.Linear(32,10)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    out = self.conv1(x)          \n",
        "    out = F.relu(out) \n",
        "    out = self.Pool1(out)\n",
        "    out = self.conv2(out)\n",
        "    out = F.relu(out) \n",
        "    out = out.view(10,64*13*13)  #Flatten\n",
        "    out = self.linear1(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = F.relu(out)\n",
        "    out = self.linear3(out)  \n",
        "    return out"
      ],
      "metadata": {
        "id": "BgRpMKphVmiw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet().to(device)"
      ],
      "metadata": {
        "id": "wopZs6azfjXH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model config\n",
        "Learning_rate = 0.001\n",
        "Loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=Learning_rate)"
      ],
      "metadata": {
        "id": "bMVDGoHUhqc3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "epochs = 5\n",
        "for epoch in range(epochs) : \n",
        "  for i, (image , labels) in enumerate(Train_loader):\n",
        "    image =image.to(device)\n",
        "    labels =labels.to(device)\n",
        "    #forward \n",
        "    output = model(image)\n",
        "    loss = Loss(output, labels)\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (i+1) % 2000 == 0:\n",
        "      print(f'epoch{epoch}/{epochs}      step : {i+1}/{len(Train_loader)}         loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGSkmNCOiNQT",
        "outputId": "f4c51e55-2f8e-4986-c640-787640f6688f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch0/5      step : 2000/5000         loss: 1.1849\n",
            "epoch0/5      step : 4000/5000         loss: 1.6130\n",
            "epoch1/5      step : 2000/5000         loss: 0.7818\n",
            "epoch1/5      step : 4000/5000         loss: 1.5834\n",
            "epoch2/5      step : 2000/5000         loss: 1.3228\n",
            "epoch2/5      step : 4000/5000         loss: 0.5385\n",
            "epoch3/5      step : 2000/5000         loss: 0.5934\n",
            "epoch3/5      step : 4000/5000         loss: 0.5324\n",
            "epoch4/5      step : 2000/5000         loss: 0.2766\n",
            "epoch4/5      step : 4000/5000         loss: 1.1584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in Test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUmIgcclWFZQ",
        "outputId": "a4d56a7a-f42a-421f-d0b0-b05ff1caffc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 83.444 %\n",
            "Accuracy of plane: 76.96151924037981 %\n",
            "Accuracy of car: 92.6040061633282 %\n",
            "Accuracy of bird: 63.18383440118285 %\n",
            "Accuracy of cat: 66.94499017681729 %\n",
            "Accuracy of deer: 83.38353413654619 %\n",
            "Accuracy of dog: 83.27661643169665 %\n",
            "Accuracy of frog: 89.02681231380338 %\n",
            "Accuracy of horse: 89.35950413223141 %\n",
            "Accuracy of ship: 95.97382989431304 %\n",
            "Accuracy of truck: 92.8535732133933 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation\n",
        "j=0\n",
        "l=0\n",
        "i=0\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in Test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        for i in range(10) : \n",
        "           if predicted[i] != labels[i] : \n",
        "              l+=1\n",
        "          \n",
        "    \n",
        "\n",
        "print(f' number of wrong predictions: {l}        accuracy: {l/n_samples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW2gydP0qGta",
        "outputId": "3a712d4e-fcca-4683-df92-225728850f57"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " number of wrong predictions: 8278        accuracy: 0.16556\n"
          ]
        }
      ]
    }
  ]
}
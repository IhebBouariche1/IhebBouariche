# -*- coding: utf-8 -*-
"""KUKA_Vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jqVJoAS6uK9keqYbhdZ1NAaWdAh1WG3
"""

#!git clone https://github.com/bulletphysics/bullet3.git

#!pip install pybullet

import os
os.chdir('C:/Users/IHEB.Home/bullet3/examples/pybullet/gym/pybullet_envs/bullet/')

#from pybullet_envs.bullet.kukaGymEnv import KukaGymEnv
import random
import os
from gym import spaces
import time
import pybullet as p
import kuka
import numpy as np
import pybullet_data
import pdb
import distutils.dir_util
import glob
from pkg_resources import parse_version
import gym
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from PIL import Image
import imageio_ffmpeg
import torchvision.transforms as T
from math import sqrt
#from google.colab import drive
import gc
from gym.utils import seeding

###########################################################################################################################
class KukaGymEnv(gym.Env):
  metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}

  def __init__(self,
               urdfRoot=pybullet_data.getDataPath(),
               actionRepeat=1,
               isEnableSelfCollision=True,
               renders=False,
               isDiscrete=False,
               maxSteps=1000):
    #print("KukaGymEnv __init__")
    self._isDiscrete = isDiscrete
    self._timeStep = 1. / 240.
    self._urdfRoot = urdfRoot
    self._actionRepeat = actionRepeat
    self._isEnableSelfCollision = isEnableSelfCollision
    self._observation = []
    self._envStepCounter = 0
    self._renders = renders
    self._maxSteps = maxSteps
    self.terminated = 0
    self._cam_dist = 1.3
    self._cam_yaw = 180
    self._cam_pitch = -40

    self._p = p
    if self._renders:
      cid = p.connect(p.SHARED_MEMORY)
      if (cid < 0):
        cid = p.connect(p.GUI)
      p.resetDebugVisualizerCamera(1.3, 180, -41, [0.52, -0.2, -0.33])
    else:
      p.connect(p.DIRECT)
    #timinglog = p.startStateLogging(p.STATE_LOGGING_PROFILE_TIMINGS, "kukaTimings.json")
    self.seed()
    self.reset()
    observationDim = len(self.getExtendedObservation())
    #print("observationDim")
    #print(observationDim)

    observation_high = np.array([largeValObservation] * observationDim)
    if (self._isDiscrete):
      self.action_space = spaces.Discrete(7)
    else:
      action_dim = 3
      self._action_bound = 1
      action_high = np.array([self._action_bound] * action_dim)
      self.action_space = spaces.Box(-action_high, action_high)
    self.observation_space = spaces.Box(-observation_high, observation_high)
    self.viewer = None

  def reset(self):
    #print("KukaGymEnv _reset")
    self.terminated = 0
    p.resetSimulation()
    p.setPhysicsEngineParameter(numSolverIterations=150)
    p.setTimeStep(self._timeStep)
    p.loadURDF(os.path.join(self._urdfRoot, "plane.urdf"), [0, 0, -1])

    p.loadURDF(os.path.join(self._urdfRoot, "table/table.urdf"), 0.5000000, 0.00000, -.820000,
               0.000000, 0.000000, 0.0, 1.0)

    xpos = 0.55 + 0.12 * random.random()
    ypos = 0 + 0.2 * random.random()
    ang = 3.14 * 0.5 + 3.1415925438 * random.random()
    orn = p.getQuaternionFromEuler([0, 0, ang])
    self.blockUid = p.loadURDF(os.path.join(self._urdfRoot, "block.urdf"), xpos, ypos, -0.15,
                               orn[0], orn[1], orn[2], orn[3])

    p.setGravity(0, 0, -10)
    self._kuka = kuka.Kuka(urdfRootPath=self._urdfRoot, timeStep=self._timeStep)
    self._envStepCounter = 0
    p.stepSimulation()
    self._observation = self.getExtendedObservation()
    return np.array(self._observation)

  def __del__(self):
    p.disconnect()

  def seed(self, seed=None):
    self.np_random, seed = seeding.np_random(seed)
    return [seed]

  def getExtendedObservation(self):
    self._observation = self._kuka.getObservation()
    gripperState = p.getLinkState(self._kuka.kukaUid, self._kuka.kukaGripperIndex)
    gripperPos = gripperState[0]
    gripperOrn = gripperState[1]
    blockPos, blockOrn = p.getBasePositionAndOrientation(self.blockUid)

    invGripperPos, invGripperOrn = p.invertTransform(gripperPos, gripperOrn)
    gripperMat = p.getMatrixFromQuaternion(gripperOrn)
    dir0 = [gripperMat[0], gripperMat[3], gripperMat[6]]
    dir1 = [gripperMat[1], gripperMat[4], gripperMat[7]]
    dir2 = [gripperMat[2], gripperMat[5], gripperMat[8]]

    gripperEul = p.getEulerFromQuaternion(gripperOrn)
    #print("gripperEul")
    #print(gripperEul)
    blockPosInGripper, blockOrnInGripper = p.multiplyTransforms(invGripperPos, invGripperOrn,
                                                                blockPos, blockOrn)
    projectedBlockPos2D = [blockPosInGripper[0], blockPosInGripper[1]]
    blockEulerInGripper = p.getEulerFromQuaternion(blockOrnInGripper)
    #print("projectedBlockPos2D")
    #print(projectedBlockPos2D)
    #print("blockEulerInGripper")
    #print(blockEulerInGripper)

    #we return the relative x,y position and euler angle of block in gripper space
    blockInGripperPosXYEulZ = [blockPosInGripper[0], blockPosInGripper[1], blockEulerInGripper[2]]

    #p.addUserDebugLine(gripperPos,[gripperPos[0]+dir0[0],gripperPos[1]+dir0[1],gripperPos[2]+dir0[2]],[1,0,0],lifeTime=1)
    #p.addUserDebugLine(gripperPos,[gripperPos[0]+dir1[0],gripperPos[1]+dir1[1],gripperPos[2]+dir1[2]],[0,1,0],lifeTime=1)
    #p.addUserDebugLine(gripperPos,[gripperPos[0]+dir2[0],gripperPos[1]+dir2[1],gripperPos[2]+dir2[2]],[0,0,1],lifeTime=1)

    self._observation.extend(list(blockInGripperPosXYEulZ))
    return self._observation

  def step(self, action):
    if (self._isDiscrete):
      dv = 0.005
      dx = [0, -dv, dv, 0, 0, 0, 0][action]
      dy = [0, 0, 0, -dv, dv, 0, 0][action]
      da = [0, 0, 0, 0, 0, -0.05, 0.05][action]
      f = 0.3
      realAction = [dx, dy, -0.002, da, f]
    else:
      #print("action[0]=", str(action[0]))
      dv = 0.005
      dx = action[0] * dv
      dy = action[1] * dv
      da = action[2] * 0.05
      f = 0.3
      realAction = [dx, dy, -0.002, da, f]
    return self.step2(realAction)

  def step2(self, action):
    for i in range(self._actionRepeat):
      self._kuka.applyAction(action)
      p.stepSimulation()
      if self._termination():
        break
      self._envStepCounter += 1
    if self._renders:
      time.sleep(self._timeStep)
    self._observation = self.getExtendedObservation()

    #print("self._envStepCounter")
    #print(self._envStepCounter)

    done = self._termination()
    npaction = np.array([
        action[3]
    ])  #only penalize rotation until learning works well [action[0],action[1],action[3]])
    actionCost = np.linalg.norm(npaction) * 10.
    #print("actionCost")
    #print(actionCost)
    reward = self._reward() - actionCost
    #print("reward")
    #print(reward)

    #print("len=%r" % len(self._observation))

    return np.array(self._observation), reward, done, {}

  def render(self, mode="rgb_array", close=False):
    if mode != "rgb_array":
      return np.array([])

    base_pos, orn = self._p.getBasePositionAndOrientation(self._kuka.kukaUid)
    view_matrix = self._p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=base_pos,
                                                            distance=self._cam_dist,
                                                            yaw=self._cam_yaw,
                                                            pitch=self._cam_pitch,
                                                            roll=0,
                                                            upAxisIndex=2)
    proj_matrix = self._p.computeProjectionMatrixFOV(fov=60,
                                                     aspect=float(RENDER_WIDTH) / RENDER_HEIGHT,
                                                     nearVal=0.1,
                                                     farVal=100.0)
    (_, _, px, _, _) = self._p.getCameraImage(width=RENDER_WIDTH,
                                              height=RENDER_HEIGHT,
                                              viewMatrix=view_matrix,
                                              projectionMatrix=proj_matrix,
                                              renderer=self._p.ER_BULLET_HARDWARE_OPENGL)
    #renderer=self._p.ER_TINY_RENDERER)

    rgb_array = np.array(px, dtype=np.uint8)
    rgb_array = np.reshape(rgb_array, (RENDER_HEIGHT, RENDER_WIDTH, 4))

    rgb_array = rgb_array[:, :, :3]
    return rgb_array

  def _termination(self):
    #print (self._kuka.endEffectorPos[2])
    state = p.getLinkState(self._kuka.kukaUid, self._kuka.kukaEndEffectorIndex)
    actualEndEffectorPos = state[0]

    #print("self._envStepCounter")
    #print(self._envStepCounter)
    if (self.terminated or self._envStepCounter > self._maxSteps):
      self._observation = self.getExtendedObservation()
      return True
    maxDist = 0.005
    closestPoints = p.getClosestPoints(self._kuka.trayUid, self._kuka.kukaUid, maxDist)

    if (len(closestPoints)):  #(actualEndEffectorPos[2] <= -0.43):
      self.terminated = 1

      #print("terminating, closing gripper, attempting grasp")
      #start grasp and terminate
      fingerAngle = 0.3
      for i in range(100):
        graspAction = [0, 0, 0.0001, 0, fingerAngle]
        self._kuka.applyAction(graspAction)
        p.stepSimulation()
        fingerAngle = fingerAngle - (0.3 / 100.)
        if (fingerAngle < 0):
          fingerAngle = 0

      for i in range(1000):
        graspAction = [0, 0, 0.001, 0, fingerAngle]
        self._kuka.applyAction(graspAction)
        p.stepSimulation()
        blockPos, blockOrn = p.getBasePositionAndOrientation(self.blockUid)
        if (blockPos[2] > 0.23):
          #print("BLOCKPOS!")
          #print(blockPos[2])
          break
        state = p.getLinkState(self._kuka.kukaUid, self._kuka.kukaEndEffectorIndex)
        actualEndEffectorPos = state[0]
        if (actualEndEffectorPos[2] > 0.5):
          break

      self._observation = self.getExtendedObservation()
      return True
    return False

  def _reward(self):

    #rewards is height of target object
    blockPos, blockOrn = p.getBasePositionAndOrientation(self.blockUid)
    closestPoints = p.getClosestPoints(self.blockUid, self._kuka.kukaUid, 1000, -1,
                                       self._kuka.kukaEndEffectorIndex)

    reward = -1000

    numPt = len(closestPoints)
    #print(numPt)
    if (numPt > 0):
      #print("reward:")
      reward = -closestPoints[0][8] * 10
    if (blockPos[2] > 0.2):
      reward = reward + 10000
      print("successfully grasped a block!!!")
      #print("self._envStepCounter")
      #print(self._envStepCounter)
      #print("self._envStepCounter")
      #print(self._envStepCounter)
      #print("reward")
      #print(reward)
    #print("reward")
    #print(reward)
    return reward

  if parse_version(gym.__version__) < parse_version('0.9.6'):
    _render = render
    _reset = reset
    _seed = seed
    _step = step

 #########################################################################################################################   
    
    

class KukaDiverseObjectEnv(KukaGymEnv):
  """Class for Kuka environment with diverse objects.
  In each episode some objects are chosen from a set of 1000 diverse objects.
  These 1000 objects are split 90/10 into a train and test set.
  """

  def __init__(self,
               urdfRoot=pybullet_data.getDataPath(),
               actionRepeat=80,
               isEnableSelfCollision=True,
               renders=False,
               isDiscrete=False,
               maxSteps=8,
               dv=0.06,
               removeHeightHack=False,
               blockRandom=0.5,
               cameraRandom=0,
               width=48,
               height=48,
               numObjects=1,
               isTest=False):
    """Initializes the KukaDiverseObjectEnv.
    Args:
      urdfRoot: The diretory from which to load environment URDF's.
      actionRepeat: The number of simulation steps to apply for each action.
      isEnableSelfCollision: If true, enable self-collision.
      renders: If true, render the bullet GUI.
      isDiscrete: If true, the action space is discrete. If False, the
        action space is continuous.
      maxSteps: The maximum number of actions per episode.
      dv: The velocity along each dimension for each action.
      removeHeightHack: If false, there is a "height hack" where the gripper
        automatically moves down for each action. If true, the environment is
        harder and the policy chooses the height displacement.
      blockRandom: A float between 0 and 1 indicated block randomness. 0 is
        deterministic.
      cameraRandom: A float between 0 and 1 indicating camera placement
        randomness. 0 is deterministic.
      width: The image width.
      height: The observation image height.
      numObjects: The number of objects in the bin.
      isTest: If true, use the test set of objects. If false, use the train
        set of objects.
    """

    self._isDiscrete = isDiscrete
    self._timeStep = 1. / 240.
    self._urdfRoot = urdfRoot
    self._actionRepeat = actionRepeat
    self._isEnableSelfCollision = isEnableSelfCollision
    self._observation = []
    self._envStepCounter = 0
    self._renders = renders
    self._maxSteps = maxSteps
    self.terminated = 0
    self._cam_dist = 1.3
    self._cam_yaw = 180
    self._cam_pitch = -40
    self._dv = dv
    self._p = p
    self._removeHeightHack = removeHeightHack
    self._blockRandom = blockRandom
    self._cameraRandom = cameraRandom
    self._width = width
    self._height = height
    self._numObjects = numObjects
    self._isTest = isTest

    if self._renders:
      self.cid = p.connect(p.SHARED_MEMORY)
      if (self.cid < 0):
        self.cid = p.connect(p.GUI)
      p.resetDebugVisualizerCamera(1.3, 180, -41, [0.52, -0.2, -0.33])
    else:
      self.cid = p.connect(p.DIRECT)
    self.seed()

    if (self._isDiscrete):
      if self._removeHeightHack:
        self.action_space = spaces.Discrete(9)
      else:
        self.action_space = spaces.Discrete(7)
    else:
      self.action_space = spaces.Box(low=-1, high=1, shape=(3,))  # dx, dy, da
      if self._removeHeightHack:
        self.action_space = spaces.Box(low=-1, high=1, shape=(4,))  # dx, dy, dz, da
    self.observation_space = spaces.Box(low=0, high=255, shape=(self._height,
                                                                self._width,
                                                                3))
    self.viewer = None

  def reset(self):
    """Environment reset called at the beginning of an episode.
    """
    # Set the camera settings.
    look = [0.23, 0.2, 0.54]
    distance = 1.
    pitch = -56 + self._cameraRandom * np.random.uniform(-3, 3)
    yaw = 245 + self._cameraRandom * np.random.uniform(-3, 3)
    roll = 0
    self._view_matrix = p.computeViewMatrixFromYawPitchRoll(look, distance, yaw, pitch, roll, 2)
    fov = 20. + self._cameraRandom * np.random.uniform(-2, 2)
    aspect = self._width / self._height
    near = 0.01
    far = 10
    self._proj_matrix = p.computeProjectionMatrixFOV(fov, aspect, near, far)

    self._attempted_grasp = False
    self._env_step = 0
    self.terminated = 0

    p.resetSimulation()
    p.setPhysicsEngineParameter(numSolverIterations=150)
    p.setTimeStep(self._timeStep)
    p.loadURDF(os.path.join(self._urdfRoot, "plane.urdf"), [0, 0, -1])

    p.loadURDF(os.path.join(self._urdfRoot, "table/table.urdf"), 0.5000000, 0.00000, -.820000,
               0.000000, 0.000000, 0.0, 1.0)

    p.setGravity(0, 0, -10)
    self._kuka = kuka.Kuka(urdfRootPath=self._urdfRoot, timeStep=self._timeStep)
    self._envStepCounter = 0
    p.stepSimulation()

    # Choose the objects in the bin.
    urdfList = self._get_random_object(self._numObjects, self._isTest)
    self._objectUids = self._randomly_place_objects(urdfList)
    self._observation = self._get_observation()
    return np.array(self._observation)

  def _randomly_place_objects(self, urdfList):
    """Randomly places the objects in the bin.
    Args:
      urdfList: The list of urdf files to place in the bin.
    Returns:
      The list of object unique ID's.
    """

    # Randomize positions of each object urdf.
    objectUids = []
    for urdf_name in urdfList:
      xpos = 0.4 + self._blockRandom * random.random()
      ypos = self._blockRandom * (random.random() - .5)
      angle = np.pi / 2 + self._blockRandom * np.pi * 1
      orn = p.getQuaternionFromEuler([0, 0, angle])
      urdf_path = os.path.join(self._urdfRoot, urdf_name)
      uid = p.loadURDF(urdf_path, [xpos, ypos, .15], [orn[0], orn[1], orn[2], orn[3]])
      objectUids.append(uid)
      # Let each object fall to the tray individual, to prevent object
      # intersection.
      for _ in range(500):
        p.stepSimulation()
    return objectUids

  def _get_observation(self):
    """Return the observation as an image.
    """
    img_arr = p.getCameraImage(width=self._width,
                               height=self._height,
                               viewMatrix=self._view_matrix,
                               projectionMatrix=self._proj_matrix)
    rgb = img_arr[2]
    np_img_arr = np.reshape(rgb, (self._height, self._width, 4))
    return np_img_arr[:, :, :3]

  def step(self, action):
    """Environment step.
    Args:
      action: 5-vector parameterizing XYZ offset, vertical angle offset
      (radians), and grasp angle (radians).
    Returns:
      observation: Next observation.
      reward: Float of the per-step reward as a result of taking the action.
      done: Bool of whether or not the episode has ended.
      debug: Dictionary of extra information provided by environment.
    """
    dv = self._dv  # velocity per physics step.
    if self._isDiscrete:
      # Static type assertion for integers.
      assert isinstance(action, int)
      if self._removeHeightHack:
        dx = [0, -dv, dv, 0, 0, 0, 0, 0, 0][action]
        dy = [0, 0, 0, -dv, dv, 0, 0, 0, 0][action]
        dz = [0, 0, 0, 0, 0, -dv, dv, 0, 0][action]
        da = [0, 0, 0, 0, 0, 0, 0, -0.25, 0.25][action]
      else:
        dx = [0, -dv, dv, 0, 0, 0, 0][action]
        dy = [0, 0, 0, -dv, dv, 0, 0][action]
        dz = -dv
        da = [0, 0, 0, 0, 0, -0.25, 0.25][action]
    else:
      dx = dv * action[0]
      dy = dv * action[1]
      if self._removeHeightHack:
        dz = dv * action[2]
        da = 0.25 * action[3]
      else:
        dz = -dv
        da = 0.25 * action[2]

    return self._step_continuous([dx, dy, dz, da, 0.3])

  def _step_continuous(self, action):
    """Applies a continuous velocity-control action.
    Args:
      action: 5-vector parameterizing XYZ offset, vertical angle offset
      (radians), and grasp angle (radians).
    Returns:
      observation: Next observation.
      reward: Float of the per-step reward as a result of taking the action.
      done: Bool of whether or not the episode has ended.
      debug: Dictionary of extra information provided by environment.
    """
    # Perform commanded action.
    self._env_step += 1
    self._kuka.applyAction(action)
    for _ in range(self._actionRepeat):
      p.stepSimulation()
      if self._renders:
        time.sleep(self._timeStep)
      if self._termination():
        break

    # If we are close to the bin, attempt grasp.
    state = p.getLinkState(self._kuka.kukaUid, self._kuka.kukaEndEffectorIndex)
    end_effector_pos = state[0]
    if end_effector_pos[2] <= 0.1:
      finger_angle = 0.3
      for _ in range(500):
        grasp_action = [0, 0, 0, 0, finger_angle]
        self._kuka.applyAction(grasp_action)
        p.stepSimulation()
        #if self._renders:
        #  time.sleep(self._timeStep)
        finger_angle -= 0.3 / 100.
        if finger_angle < 0:
          finger_angle = 0
      for _ in range(500):
        grasp_action = [0, 0, 0.001, 0, finger_angle]
        self._kuka.applyAction(grasp_action)
        p.stepSimulation()
        if self._renders:
          time.sleep(self._timeStep)
        finger_angle -= 0.3 / 100.
        if finger_angle < 0:
          finger_angle = 0
      self._attempted_grasp = True
    observation = self._get_observation()
    done = self._termination()
    reward = self._reward()

    debug = {'grasp_success': self._graspSuccess}
    return observation, reward, done, debug

  def _reward(self):
    """Calculates the reward for the episode.
    The reward is 1 if one of the objects is above height .2 at the end of the
    episode.
    """
    reward = 0
    self._graspSuccess = 0
    for uid in self._objectUids:
      pos, _ = p.getBasePositionAndOrientation(uid)
      # If any block is above height, provide reward.
      if pos[2] > 0.2:
        self._graspSuccess += 1
        reward = 1
        break
    return reward

  def _termination(self):
    """Terminates the episode if we have tried to grasp or if we are above
    maxSteps steps.
    """
    return self._attempted_grasp or self._env_step >= self._maxSteps

  def _get_random_object(self, num_objects, test):
    """Randomly choose an object urdf from the random_urdfs directory.
    Args:
      num_objects:
        Number of graspable objects.
    Returns:
      A list of urdf filenames.
    """
    if test:
      urdf_pattern = os.path.join(self._urdfRoot, 'random_urdfs/*0/*.urdf')
    else:
      urdf_pattern = os.path.join(self._urdfRoot, 'random_urdfs/*[1-9]/*.urdf')
    found_object_directories = glob.glob(urdf_pattern)
    total_num_objects = len(found_object_directories)
    selected_objects = np.random.choice(np.arange(total_num_objects)[15:16], num_objects)
    selected_objects_filenames = []
    for object_index in selected_objects:
      selected_objects_filenames += [found_object_directories[object_index]]
    return selected_objects_filenames

  if parse_version(gym.__version__) < parse_version('0.9.6'):
    _reset = reset
    _step = step

class Buffer():
  def __init__(self,Buffer_size):
    self.Buffer_size = Buffer_size
    self.states = []
    self.actions = []
    self.rewards = []
    self.next_states = []
    self.dones = []
  
  def Add_(self,state,action,reward,next_state,done):
    if len(self.states) > self.Buffer_size-1: 
      del(self.states[0])
      del(self.actions[0])
      del(self.rewards[0])
      del(self.next_states[0])
      del(self.dones[0])
    self.states.append(state)
    self.actions.append(action)
    self.rewards.append(reward)
    self.next_states.append(next_state)
    self.dones.append(done)
    return self.states, self.actions, self.rewards, self.next_states, self.dones

class Actor(nn.Module): 
  def __init__(self,state_size,action_dim):
    super(Actor,self).__init__()

    self.state_size = (state_size[0],state_size[1])

    #Convolutional neural network
    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
    self.bn1 = nn.BatchNorm2d(16)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
    self.bn2 = nn.BatchNorm2d(32)
    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
    self.bn3 = nn.BatchNorm2d(32)
   
    self.Tanh = nn.Tanh()
    self.ReLU = nn.ReLU()


    def conv2d_size_out(size, kernel_size = 5, stride = 2):
      return (size - (kernel_size - 1) - 1) // stride  + 1

    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[0])))
    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[1])))
    linear_input_size = convh * convw * 32

    self.sharedlayers = nn.Sequential(
        nn.Linear(linear_input_size,400),
        nn.ReLU())
    
    #Actor network 
    self.actor = nn.Sequential(
          nn.Linear(400,400),
          nn.ReLU(),
          nn.Linear(400,400),
          nn.ReLU(),
          nn.Linear(400,action_dim),
          nn.Tanh())
  
  def sharedforward(self,state):
    Out = self.ReLU(self.bn1(self.conv1(state)))
    Out = self.ReLU(self.bn2(self.conv2(Out)))
    Out = self.ReLU(self.bn3(self.conv3(Out)))
    Out = self.sharedlayers(Out.view(Out.size(0), -1))
    return Out
    
  def forward(self,state):
    action = self.actor(self.sharedforward(state))
    return action 
    
  def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

class Critic(nn.Module): 
  def __init__(self,state_size,action_dim):
    super(Critic,self).__init__()

    self.state_size = (state_size[0],state_size[1])

    #Convolutional neural network
    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)
    self.bn1 = nn.BatchNorm2d(16)
    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)
    self.bn2 = nn.BatchNorm2d(32)
    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)
    self.bn3 = nn.BatchNorm2d(32)
   
    self.Tanh= nn.Tanh()
    self.ReLU = nn.ReLU()

    def conv2d_size_out(size, kernel_size = 5, stride = 2):
      return (size - (kernel_size - 1) - 1) // stride  + 1

    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[0])))
    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(self.state_size[1])))
    linear_input_size = convh * convw * 32

    self.sharedlayers = nn.Sequential(
        nn.Linear(linear_input_size,400),
        nn.ReLU())
    
    #Critic network 
    self.critic_action = nn.Sequential(
        nn.Linear(action_dim,400),
        nn.ReLU())
    
    self.critic_state = nn.Sequential(
        nn.Linear(400,400),
        nn.ReLU(),
        nn.Linear(400,400),
        nn.ReLU())
    
    self.critic = nn.Sequential(
        nn.Linear(800,400),
        nn.ReLU(),
        nn.Linear(400,1),
        nn.Tanh())
    
  def sharedforward(self,state):
    Out = self.ReLU(self.bn1(self.conv1(state)))
    Out = self.ReLU(self.bn2(self.conv2(Out)))
    Out = self.ReLU(self.bn3(self.conv3(Out)))
    Out = self.sharedlayers(Out.view(Out.size(0), -1))
    return Out
    
  def forward(self,state,action):
    state_critic = self.critic_state(self.sharedforward(state))
    action_critic = self.critic_action(action)
    value = self.critic(torch.cat((state_critic, action_critic), dim=1)) 
    return value
    
  def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

class DDPG():
  def __init__(self,state_size,action_dim,Actor_learning_rate,Critic_learning_rate,discount_factor,buffer_size):
    self.discount_factor = discount_factor
    self.Actor_learning_rate = Actor_learning_rate
    self.Critic_learning_rate = Critic_learning_rate
    self.state_size = state_size 
    self.action_dim = action_dim
    self.ActorTarget = Actor(self.state_size,self.action_dim)
    self.Actor = Actor(self.state_size,self.action_dim)
    self.CriticTarget = Critic(self.state_size,self.action_dim)
    self.Critic = Critic(self.state_size,self.action_dim)
    self.Buffer = Buffer(buffer_size)
    self.MSE = nn.MSELoss()
    self.actor_optimizer = torch.optim.Adam(self.Actor.parameters(),lr = self.Actor_learning_rate)
    self.critic_optimizer = torch.optim.Adam(self.Critic.parameters(),lr = self.Critic_learning_rate)
  
  def ClearBuffer(self):
     self.Buffer.states = []
     self.Buffer.actions = []
     self.Buffer.rewards = []
     self.Buffer.next_states = []
     self.Buffer.dones = []


  #Ornstein-Uhlenbeck Noise
  def Noise(self,noise_std,noise_decay_rate):
    
    noise = noise_std*torch.tensor([random.uniform(-1, 1),random.uniform(-1, 1),random.uniform(-1, 1)]).to(device)
    noise_std = noise_std*(1 - noise_decay_rate)

    return noise_std,noise






  def step(self,env,state,noise):
    action = self.Actor.forward(state)
    #Action used to learn 
    action = action 
    _,reward,done,_ = env.step(action[0])
    if reward : 
      reward = 1
    if done == 1 and reward == 0: 
      reward = -1    
    next_state = get_screen(env)
    self.Buffer.Add_(state,action,reward,next_state,done)
    return self.Buffer.states,self.Buffer.actions,self.Buffer.rewards,self.Buffer.next_states,self.Buffer.dones
  
    
    
  def step_noise(self,env,state,noise):
    action = noise.unsqueeze(0)
    _,reward,done,_ = env.step(noise)
    if reward : 
      reward = 1
    if done == 1 and reward == 0: 
      reward = -1    
    next_state = get_screen(env)
    self.Buffer.Add_(state,action,reward,next_state,done)
    return self.Buffer.states,self.Buffer.actions,self.Buffer.rewards,self.Buffer.next_states,self.Buffer.dones
    
    
  #Selecting shuffle of mini batch in the buffer memory
  def Mini_batch(self,states,actions,rewards,next_states,dones,Mini_Batch_Size):
    batch_states = []
    batch_actions = []
    batch_rewards = []
    batch_next_states = []
    batch_dones = []
    indxs = [i for i in range(len(rewards))]
    np.random.shuffle(indxs)
    if len(rewards) > Mini_Batch_Size :
      batch_states = [states[ind] for ind in indxs[0:Mini_Batch_Size]] 
      batch_actions = [actions[ind] for ind in indxs[0:Mini_Batch_Size]] 
      batch_rewards = [rewards[ind] for ind in indxs[0:Mini_Batch_Size]] 
      batch_next_states = [next_states[ind] for ind in indxs[0:Mini_Batch_Size]]
      batch_dones = [dones[ind] for ind in indxs[0:Mini_Batch_Size]]
    else: 
      batch_states = states
      batch_actions = actions
      batch_rewards = rewards
      batch_next_states = next_states
      batch_dones = dones
    batch_states = torch.cat(batch_states, dim=0)
    batch_actions = torch.cat(batch_actions, dim=0)
    batch_rewards = torch.tensor(np.array(batch_rewards).reshape((-1, 1)))
    batch_next_states = torch.cat(batch_next_states, dim=0)
    return batch_states,batch_actions,batch_rewards,batch_next_states,batch_dones

  def update(self,batch_states,batch_actions,batch_rewards,batch_next_states,batch_dones):
    actions_target = self.ActorTarget.forward(batch_next_states).detach()
    values_target = self.CriticTarget.forward(batch_next_states,actions_target).detach()
    #Target value 
    for i,done in enumerate(batch_dones):
      if done: 
        values_target[i] = 0 
         
    y = batch_rewards.unsqueeze(0).to(device) + self.discount_factor * values_target
    
    #Critic update
    detached_action = batch_actions.detach()
    Values_critic = self.Critic.forward(batch_states,detached_action)
    Critic_LOSS = self.MSE(y,Values_critic)  
    #update the critic newtwork
    self.critic_optimizer.zero_grad()
    Critic_LOSS.backward(retain_graph = True)
    #torch.nn.utils.clip_grad_norm_(self.Critic.parameters(),1)
    self.critic_optimizer.step()
  
  
    #Actor update
    actions = self.Actor.forward(batch_states)
    Values_critic1 = self.Critic.forward(batch_states,actions)
    Actor_LOSS = -torch.mean(Values_critic1)
    #update the actor network
    self.actor_optimizer.zero_grad()
    Actor_LOSS.backward(retain_graph = True)
    #torch.nn.utils.clip_grad_norm_(self.Actor.parameters(),1)
    self.actor_optimizer.step()
    Values_critic2 = self.Critic.forward(batch_states,actions)
    actions1 = self.Actor.forward(batch_states)
    return Actor_LOSS, Critic_LOSS,actions_target,values_target,y,Values_critic,Values_critic1,Values_critic2,actions,actions1

  def update_target(self):
    self.ActorTarget.load_state_dict(self.Actor.state_dict())
    self.CriticTarget.load_state_dict(self.Critic.state_dict())

#Preprocessing of images
resize = T.Compose([T.ToPILImage(),
                    T.Resize(40, interpolation=Image.CUBIC),
                    T.ToTensor()])

def get_screen(env):
    # Returned screen requested by gym is 400x600x3, but is sometimes larger
    # such as 800x1200x3. Transpose it into torch order (CHW).
    #env.render(mode='human')
    screen = env._get_observation().transpose((2, 0, 1))
    # Convert to float, rescale, convert to torch tensor
    # (this doesn't require a copy)
    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
    screen = torch.from_numpy(screen)
    # Resize, and add a batch dimension (BCHW)
    return resize(screen).unsqueeze(0).to(device)

env = KukaDiverseObjectEnv(renders=True, isDiscrete=False, removeHeightHack=False,  actionRepeat=80, maxSteps = 20,numObjects = 1,blockRandom=0.2 )
#env.cid = p.connect(p.DIRECT)
action_space = spaces.Box(low=-1, high=1, shape=(5,1))
# if gpu is to be used
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
env.reset()
#drive.mount('/content/drive')

#The effecor camera image 
env.reset()
plt.figure()
image = get_screen(env)
#plt.imshow(image.cpu().squeeze(0).permute(1, 2, 0).numpy(),
#           interpolation='none')
#plt.title('Example extracted screen')
#plt.show()

state = get_screen(env)
state_size = (state.shape[2],state.shape[3])
action_dim = env.action_space.shape[0]

max_episodes = 1000000
max_steps = 50
saving_rate = 100
Target_updating_rate = 3
Ploting_rate = 100
MiniBatch_size = 128

d_rewards=[]
d_ActorLOSS=[]
d_CriticLOSS=[]
AL = 0
CL = 0
s = 0
d = 0
k = 0

Policy = DDPG(state_size,action_dim,Actor_learning_rate = 0.0001,Critic_learning_rate = 0.001,discount_factor = 0.98,buffer_size = 1e6)

# Update policies 



import torch
import io



# Open the file as a buffer
with open("Critic.pt", 'rb') as f:
    buffer = io.BytesIO(f.read())
# Load the model from the buffer
state_dict = torch.load("Critic.pt")
Policy.Critic.load_state_dict(state_dict)


# Open the file as a buffer
with open("Actor.pt", 'rb') as f:
    buffer = io.BytesIO(f.read())
# Load the model from the buffer
state_dict = torch.load("Actor.pt")
Policy.Actor.load_state_dict(state_dict)


Policy.update_target()


if torch.cuda.is_available():
    Policy.ActorTarget.cuda()
    Policy.Actor.cuda()
    Policy.CriticTarget.cuda()
    Policy.Critic.cuda()

noise = 0

for e in range(max_episodes): 
  env.reset()
  for step in range(max_steps): 

    state = get_screen(env)    
    states,actions,rewards,next_states,dones = Policy.step(env,state,noise)


    #print("episode: ",s, " reward: ",rewards[-1], " Actor Loss: ",Actor_Loss,  " Critic Loss: ",Critic_Loss) 
    if rewards[-1] == 1:
      d = d + 1

    if (e+1) % 1 == 0 and dones[-1] == 1:
      f = s - k
      print("step",f,"episode: ",e+1, " reward: ",rewards[-1]) 
      AL = 0
      CL = 0
      k = s 

    #Saving data on GOOGLE DRIVE 
    if (e+1)%saving_rate==0 and dones[-1] == 1:
      print("===============================================SAVED======================================================= \n")
      print(" Average reward on 100 episode: ", d/saving_rate, "    number of rewards:    ",d,"\n")
      print("===========================================================================================================")
      d = 0
    if dones[-1]: 
      break
